{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10672\n",
      "1334\n",
      "1334\n"
     ]
    }
   ],
   "source": [
    "#80\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "train = pd.read_csv(\"../data/train.txt\", sep=\"\\t\")\n",
    "print(len(train))\n",
    "valid = pd.read_csv(\"../data/valid.txt\", sep='\\t')\n",
    "print(len(valid))\n",
    "test = pd.read_csv(\"../data/test.txt\", sep=\"\\t\")\n",
    "print(len(test))\n",
    "\n",
    "def add_words(df):\n",
    "    df[\"words\"] = df.TITLE.apply(lambda x: x.split(\" \"))\n",
    "    return df\n",
    "\n",
    "train = add_words(train)\n",
    "valid = add_words(valid)\n",
    "test = add_words(test)\n",
    "\n",
    "\n",
    "def make_dict(df):\n",
    "    frequency = defaultdict(int)\n",
    "    for text in df.words:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "    return frequency\n",
    "\n",
    "\n",
    "def make_id_dict(dic):\n",
    "    id_dict={}\n",
    "    for i , (k,v) in enumerate(sorted(dic.items(), key=lambda x : -x[1])):\n",
    "    #軽量化のためv>=5に。本来はv>=2\n",
    "        if v>=2:\n",
    "            id_dict[k]=i+1\n",
    "        else:\n",
    "            id_dict[k]=0\n",
    "    return id_dict\n",
    "\n",
    "word_dict=make_id_dict(make_dict(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalize_class(df):\n",
    "    size=min(df.CATEGORY.value_counts())\n",
    "    result=[]\n",
    "    for category in df.CATEGORY.unique():\n",
    "        result.append(df[df.CATEGORY==category].iloc[:size])\n",
    "    \n",
    "    return pd.concat(result).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_to_vec(df, id_dic):\n",
    "    dim=max(word_dict.values())+1\n",
    "    result=torch.zeros([df.shape[0], df.shape[1]], dtype=torch.long) \n",
    "    for i, sentence in enumerate(df):\n",
    "        for u, word in enumerate(sentence):\n",
    "            try:\n",
    "                result[i, u]=id_dic[word]\n",
    "            except:\n",
    "                continue\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "#81\n",
    "train = equalize_class(train)\n",
    "valid = equalize_class(valid)\n",
    "X_train=id_to_vec(train, word_dict)\n",
    "X_valid=id_to_vec(valid, word_dict)\n",
    "#X_test=id_to_vec(uniform_sentence_len(test), word_dict)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#LabelEncoderのインスタンスを生成\n",
    "le = LabelEncoder()\n",
    "#ラベルを覚えさせる\n",
    "le = le.fit(train.CATEGORY.values)\n",
    "#ラベルを整数に変換\n",
    "\n",
    "y_train = le.transform(train.CATEGORY.values)\n",
    "y_valid = le.transform(valid.CATEGORY.values)\n",
    "#y_test = le.transform(test.CATEGORY.values)\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "def get_acc(y_hat, y):\n",
    "    _, label=torch.max(y_hat, 1)\n",
    "    correct=(label==y).sum().item()\n",
    "    return correct/label.size(0)\n",
    "\n",
    "VOCAB_SIZE = max(word_dict.values())+1\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "\n",
    "#X_test = to_tensor(X_test)\n",
    "y_train = torch.tensor(y_train , dtype=torch.long)\n",
    "y_valid = torch.tensor(y_valid, dtype=torch.long)\n",
    "#y_test = to_tensor(y_test).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, dim_w, dim_h, L, vocab_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.dim_h=dim_h\n",
    "        self.emb = nn.Embedding(vocab_size, dim_w,\n",
    "                                padding_idx=0)\n",
    "        self.rnn = nn.LSTM(dim_w, dim_h, 1, batch_first=True)\n",
    "        self.out = nn.Linear(dim_h, L)\n",
    "        self.softmax =nn.Softmax(1)\n",
    "    def forward(self, x, h0=None):\n",
    "        x = self.emb(x)\n",
    "        x, h = self.rnn(x, h0)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.out(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/30 epoch train_loss:1.3485 | train_acc:0.2500 | valid_loss:1.3485 | valid_acc:0.2500\n",
      "2/30 epoch train_loss:1.3347 | train_acc:0.2500 | valid_loss:1.3347 | valid_acc:0.2500\n",
      "3/30 epoch train_loss:1.3277 | train_acc:0.2500 | valid_loss:1.3277 | valid_acc:0.2500\n",
      "4/30 epoch train_loss:1.3236 | train_acc:0.2500 | valid_loss:1.3235 | valid_acc:0.2500\n",
      "5/30 epoch train_loss:1.3209 | train_acc:0.2500 | valid_loss:1.3209 | valid_acc:0.2525\n",
      "6/30 epoch train_loss:1.3192 | train_acc:0.2500 | valid_loss:1.3191 | valid_acc:0.2525\n",
      "7/30 epoch train_loss:1.3179 | train_acc:0.2510 | valid_loss:1.3179 | valid_acc:0.2500\n",
      "8/30 epoch train_loss:1.3170 | train_acc:0.2510 | valid_loss:1.3170 | valid_acc:0.2500\n",
      "9/30 epoch train_loss:1.3163 | train_acc:0.2503 | valid_loss:1.3163 | valid_acc:0.2500\n",
      "10/30 epoch train_loss:1.3158 | train_acc:0.2507 | valid_loss:1.3158 | valid_acc:0.2500\n",
      "11/30 epoch train_loss:1.3154 | train_acc:0.2507 | valid_loss:1.3154 | valid_acc:0.2500\n",
      "12/30 epoch train_loss:1.3151 | train_acc:0.2500 | valid_loss:1.3150 | valid_acc:0.2551\n",
      "13/30 epoch train_loss:1.3148 | train_acc:0.2500 | valid_loss:1.3148 | valid_acc:0.2525\n",
      "14/30 epoch train_loss:1.3146 | train_acc:0.2500 | valid_loss:1.3146 | valid_acc:0.2525\n",
      "15/30 epoch train_loss:1.3144 | train_acc:0.2500 | valid_loss:1.3144 | valid_acc:0.2525\n",
      "16/30 epoch train_loss:1.3143 | train_acc:0.2500 | valid_loss:1.3142 | valid_acc:0.2525\n",
      "17/30 epoch train_loss:1.3141 | train_acc:0.2500 | valid_loss:1.3141 | valid_acc:0.2525\n",
      "18/30 epoch train_loss:1.3140 | train_acc:0.2500 | valid_loss:1.3140 | valid_acc:0.2525\n",
      "19/30 epoch train_loss:1.3139 | train_acc:0.2500 | valid_loss:1.3139 | valid_acc:0.2525\n",
      "20/30 epoch train_loss:1.3138 | train_acc:0.2500 | valid_loss:1.3138 | valid_acc:0.2525\n",
      "21/30 epoch train_loss:1.3138 | train_acc:0.2497 | valid_loss:1.3137 | valid_acc:0.2525\n",
      "22/30 epoch train_loss:1.3137 | train_acc:0.2497 | valid_loss:1.3137 | valid_acc:0.2525\n",
      "23/30 epoch train_loss:1.3136 | train_acc:0.2497 | valid_loss:1.3136 | valid_acc:0.2525\n",
      "24/30 epoch train_loss:1.3136 | train_acc:0.2497 | valid_loss:1.3136 | valid_acc:0.2525\n",
      "25/30 epoch train_loss:1.3135 | train_acc:0.2497 | valid_loss:1.3135 | valid_acc:0.2525\n",
      "26/30 epoch train_loss:1.3135 | train_acc:0.2497 | valid_loss:1.3135 | valid_acc:0.2525\n",
      "27/30 epoch train_loss:1.3135 | train_acc:0.2497 | valid_loss:1.3134 | valid_acc:0.2525\n",
      "28/30 epoch train_loss:1.3134 | train_acc:0.2497 | valid_loss:1.3134 | valid_acc:0.2525\n",
      "29/30 epoch train_loss:1.3134 | train_acc:0.2497 | valid_loss:1.3134 | valid_acc:0.2525\n",
      "30/30 epoch train_loss:1.3134 | train_acc:0.2497 | valid_loss:1.3133 | valid_acc:0.2525\n"
     ]
    }
   ],
   "source": [
    "model = RNN(EMBEDDING_DIM,100,4, VOCAB_SIZE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.SGD(params=model.parameters() , lr=0.1)\n",
    "optimizer.zero_grad()\n",
    "epochs=30\n",
    "batch_size=32\n",
    "for epoch in range(epochs):\n",
    "    total_loss=0\n",
    "    for idx in range(0, len(X_train), batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        hat_y = model(X_train[idx : idx+batch_size if idx+batch_size<=len(X_train) else len(X_train)], None)\n",
    "        loss=criterion(hat_y, y_train[idx : idx+batch_size if idx+batch_size<=len(X_train) else len(X_train)])\n",
    "        total_loss+=loss.item()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "    #print(total_loss)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hat_train_y=model(X_train)\n",
    "        train_loss=criterion(hat_train_y, y_train)\n",
    "        train_acc=get_acc(hat_train_y,y_train)\n",
    "\n",
    "        #検証用\n",
    "        hat_valid_y = model(X_valid)\n",
    "        valid_loss=criterion(hat_valid_y, y_valid)\n",
    "        valid_acc=get_acc(hat_valid_y,y_valid)\n",
    "        print(\"{}/{} epoch train_loss:{:.4f} | train_acc:{:.4f} | valid_loss:{:.4f} | valid_acc:{:.4f}\".format(\n",
    "            epoch+1, epochs, train_loss, train_acc, valid_loss, valid_acc))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#84\n",
    "import gensim\n",
    "file_path = \"../data/GoogleNews-vectors-negative300.bin.gz\"\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(file_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.rnn as rnn\n",
    "def get_pre_trained_vec(df):\n",
    "    result=[]\n",
    "    for sentence in df.words:\n",
    "        sentence_list=[]\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                sentence_list.append(word2vec[word])\n",
    "            except:\n",
    "                continue\n",
    "        if sentence_list!=[]:\n",
    "            result.append(torch.tensor(sentence_list))\n",
    "        else:\n",
    "            result.append(torch.zeros(1,1,300))\n",
    "    return rnn.pad_sequence(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = equalize_class(train)\n",
    "valid = equalize_class(valid)\n",
    "\n",
    "\n",
    "X_train=get_pre_trained_vec(train).reshape(len(train), -1,300)\n",
    "X_valid=get_pre_trained_vec(valid).reshape(len(valid), -1,300)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#LabelEncoderのインスタンスを生成\n",
    "le = LabelEncoder()\n",
    "#ラベルを覚えさせる\n",
    "le = le.fit(train.CATEGORY.values)\n",
    "#ラベルを整数に変換\n",
    "\n",
    "y_train = le.transform(train.CATEGORY.values)\n",
    "y_valid = le.transform(valid.CATEGORY.values)\n",
    "y_train = torch.tensor(y_train , dtype=torch.long)\n",
    "y_valid = torch.tensor(y_valid, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bi_RNN(nn.Module):\n",
    "    def __init__(self, dim_w, dim_h, L):\n",
    "        super(Bi_RNN, self).__init__()\n",
    "        self.rnn = nn.LSTM(dim_w, dim_h, 1, batch_first=True, bidirectional=True)\n",
    "        self.out = nn.Linear(2*dim_h, L)\n",
    "        self.softmax =nn.Softmax(1)\n",
    "    def forward(self, x, h0=None):\n",
    "        x, h = self.rnn(x, h0)\n",
    "        x = x[:,-1,:]\n",
    "        x = self.out(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/30 epoch train_loss:1.3732 | train_acc:0.2493 | valid_loss:1.3731 | valid_acc:0.2500\n",
      "2/30 epoch train_loss:1.3611 | train_acc:0.2486 | valid_loss:1.3610 | valid_acc:0.2500\n",
      "3/30 epoch train_loss:1.3524 | train_acc:0.2490 | valid_loss:1.3523 | valid_acc:0.2500\n",
      "4/30 epoch train_loss:1.3459 | train_acc:0.2497 | valid_loss:1.3459 | valid_acc:0.2525\n",
      "5/30 epoch train_loss:1.3410 | train_acc:0.2493 | valid_loss:1.3410 | valid_acc:0.2551\n",
      "6/30 epoch train_loss:1.3371 | train_acc:0.2500 | valid_loss:1.3371 | valid_acc:0.2576\n",
      "7/30 epoch train_loss:1.3339 | train_acc:0.2503 | valid_loss:1.3340 | valid_acc:0.2551\n",
      "8/30 epoch train_loss:1.3314 | train_acc:0.2514 | valid_loss:1.3315 | valid_acc:0.2500\n",
      "9/30 epoch train_loss:1.3292 | train_acc:0.2552 | valid_loss:1.3294 | valid_acc:0.2424\n",
      "10/30 epoch train_loss:1.3275 | train_acc:0.2576 | valid_loss:1.3277 | valid_acc:0.2399\n",
      "11/30 epoch train_loss:1.3260 | train_acc:0.2562 | valid_loss:1.3263 | valid_acc:0.2399\n",
      "12/30 epoch train_loss:1.3247 | train_acc:0.2535 | valid_loss:1.3250 | valid_acc:0.2399\n",
      "13/30 epoch train_loss:1.3236 | train_acc:0.2576 | valid_loss:1.3240 | valid_acc:0.2475\n",
      "14/30 epoch train_loss:1.3226 | train_acc:0.2594 | valid_loss:1.3230 | valid_acc:0.2626\n",
      "15/30 epoch train_loss:1.3218 | train_acc:0.2722 | valid_loss:1.3222 | valid_acc:0.2298\n",
      "16/30 epoch train_loss:1.3210 | train_acc:0.2677 | valid_loss:1.3215 | valid_acc:0.2348\n",
      "17/30 epoch train_loss:1.3203 | train_acc:0.2694 | valid_loss:1.3209 | valid_acc:0.2323\n",
      "18/30 epoch train_loss:1.3198 | train_acc:0.2719 | valid_loss:1.3204 | valid_acc:0.2374\n",
      "19/30 epoch train_loss:1.3192 | train_acc:0.2712 | valid_loss:1.3199 | valid_acc:0.2475\n",
      "20/30 epoch train_loss:1.3187 | train_acc:0.2694 | valid_loss:1.3195 | valid_acc:0.2424\n",
      "21/30 epoch train_loss:1.3183 | train_acc:0.2698 | valid_loss:1.3191 | valid_acc:0.2374\n",
      "22/30 epoch train_loss:1.3179 | train_acc:0.2684 | valid_loss:1.3188 | valid_acc:0.2475\n",
      "23/30 epoch train_loss:1.3176 | train_acc:0.2684 | valid_loss:1.3184 | valid_acc:0.2475\n",
      "24/30 epoch train_loss:1.3173 | train_acc:0.2705 | valid_loss:1.3182 | valid_acc:0.2374\n",
      "25/30 epoch train_loss:1.3169 | train_acc:0.2726 | valid_loss:1.3179 | valid_acc:0.2399\n",
      "26/30 epoch train_loss:1.3167 | train_acc:0.2726 | valid_loss:1.3177 | valid_acc:0.2399\n",
      "27/30 epoch train_loss:1.3164 | train_acc:0.2719 | valid_loss:1.3175 | valid_acc:0.2374\n",
      "28/30 epoch train_loss:1.3162 | train_acc:0.2708 | valid_loss:1.3173 | valid_acc:0.2348\n",
      "29/30 epoch train_loss:1.3160 | train_acc:0.2726 | valid_loss:1.3171 | valid_acc:0.2298\n",
      "30/30 epoch train_loss:1.3158 | train_acc:0.2715 | valid_loss:1.3169 | valid_acc:0.2298\n"
     ]
    }
   ],
   "source": [
    "#84#85\n",
    "model = Bi_RNN(300,50,4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.SGD(params=model.parameters() , lr=0.03)\n",
    "optimizer.zero_grad()\n",
    "epochs=30\n",
    "batch_size=32\n",
    "for epoch in range(epochs):\n",
    "    total_loss=0\n",
    "    for idx in range(0, len(X_train), batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        hat_y = model(X_train[idx : idx+batch_size if idx+batch_size<=len(X_train) else len(X_train)], None)\n",
    "        loss=criterion(hat_y, y_train[idx : idx+batch_size if idx+batch_size<=len(X_train) else len(X_train)])\n",
    "        total_loss+=loss.item()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        hat_y_train = model(X_train)\n",
    "        hat_y_valid = model(X_valid)\n",
    "        train_acc=get_acc(hat_y_train, y_train)\n",
    "        valid_acc=get_acc(hat_y_valid, y_valid)\n",
    "        train_loss = criterion(hat_y_train, y_train)\n",
    "        valid_loss = criterion(hat_y_valid, y_valid)\n",
    "        print(\"{}/{} epoch train_loss:{:.4f} | train_acc:{:.4f} | valid_loss:{:.4f} | valid_acc:{:.4f}\".format(\n",
    "            epoch+1, epochs, train_loss, train_acc, valid_loss, valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "#87\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, dim_w, dim_h, L):\n",
    "        super(CNN, self).__init__()\n",
    "        self.dim_h=dim_h\n",
    "        self.cnn1 = nn.Conv1d(dim_w, dim_h,3, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(dim_h, dim_h, 3, padding=1)\n",
    "        self.max_pooling= nn.MaxPool1d(2)\n",
    "        self.fc = nn.Linear(dim_h, L)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn1(x)\n",
    "        x = self.max_pooling(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = torch.max(x, 2).values\n",
    "        x = self.fc(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train=X_train.view(len(X_train), 300, -1)\n",
    "X_valid=X_valid.view(len(X_valid), 300, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/30 epoch train_loss:1.3381 | train_acc:0.2684 | valid_loss:1.3378 | valid_acc:0.2677\n",
      "2/30 epoch train_loss:1.3234 | train_acc:0.2983 | valid_loss:1.3239 | valid_acc:0.2551\n",
      "3/30 epoch train_loss:1.3176 | train_acc:0.2639 | valid_loss:1.3188 | valid_acc:0.2551\n",
      "4/30 epoch train_loss:1.3147 | train_acc:0.2524 | valid_loss:1.3167 | valid_acc:0.2500\n",
      "5/30 epoch train_loss:1.3128 | train_acc:0.2500 | valid_loss:1.3157 | valid_acc:0.2500\n",
      "6/30 epoch train_loss:1.3114 | train_acc:0.2503 | valid_loss:1.3153 | valid_acc:0.2500\n",
      "7/30 epoch train_loss:1.3101 | train_acc:0.2514 | valid_loss:1.3151 | valid_acc:0.2500\n",
      "8/30 epoch train_loss:1.3088 | train_acc:0.2538 | valid_loss:1.3150 | valid_acc:0.2500\n",
      "9/30 epoch train_loss:1.3075 | train_acc:0.2587 | valid_loss:1.3151 | valid_acc:0.2500\n",
      "10/30 epoch train_loss:1.3060 | train_acc:0.2705 | valid_loss:1.3153 | valid_acc:0.2475\n",
      "11/30 epoch train_loss:1.3043 | train_acc:0.2951 | valid_loss:1.3155 | valid_acc:0.2500\n",
      "12/30 epoch train_loss:1.3024 | train_acc:0.3215 | valid_loss:1.3158 | valid_acc:0.2525\n",
      "13/30 epoch train_loss:1.3000 | train_acc:0.3444 | valid_loss:1.3162 | valid_acc:0.2576\n",
      "14/30 epoch train_loss:1.2973 | train_acc:0.3694 | valid_loss:1.3166 | valid_acc:0.2576\n",
      "15/30 epoch train_loss:1.2940 | train_acc:0.3802 | valid_loss:1.3172 | valid_acc:0.2551\n",
      "16/30 epoch train_loss:1.2900 | train_acc:0.3830 | valid_loss:1.3179 | valid_acc:0.2399\n",
      "17/30 epoch train_loss:1.2850 | train_acc:0.3816 | valid_loss:1.3188 | valid_acc:0.2399\n",
      "18/30 epoch train_loss:1.2790 | train_acc:0.3778 | valid_loss:1.3200 | valid_acc:0.2222\n",
      "19/30 epoch train_loss:1.2719 | train_acc:0.3722 | valid_loss:1.3215 | valid_acc:0.2172\n",
      "20/30 epoch train_loss:1.2634 | train_acc:0.3618 | valid_loss:1.3235 | valid_acc:0.2071\n",
      "21/30 epoch train_loss:1.2538 | train_acc:0.3587 | valid_loss:1.3262 | valid_acc:0.2020\n",
      "22/30 epoch train_loss:1.2434 | train_acc:0.3594 | valid_loss:1.3298 | valid_acc:0.1995\n",
      "23/30 epoch train_loss:1.2326 | train_acc:0.3681 | valid_loss:1.3342 | valid_acc:0.2172\n",
      "24/30 epoch train_loss:1.2217 | train_acc:0.3750 | valid_loss:1.3396 | valid_acc:0.2172\n",
      "25/30 epoch train_loss:1.2100 | train_acc:0.3868 | valid_loss:1.3454 | valid_acc:0.2172\n",
      "26/30 epoch train_loss:1.1972 | train_acc:0.3997 | valid_loss:1.3516 | valid_acc:0.2197\n",
      "27/30 epoch train_loss:1.1814 | train_acc:0.4240 | valid_loss:1.3576 | valid_acc:0.2172\n",
      "28/30 epoch train_loss:1.1636 | train_acc:0.4424 | valid_loss:1.3639 | valid_acc:0.2146\n",
      "29/30 epoch train_loss:1.1416 | train_acc:0.4691 | valid_loss:1.3693 | valid_acc:0.2146\n",
      "30/30 epoch train_loss:1.1180 | train_acc:0.4951 | valid_loss:1.3747 | valid_acc:0.2121\n"
     ]
    }
   ],
   "source": [
    "model = CNN(300,100,4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.SGD(params=model.parameters() , lr=0.1)\n",
    "optimizer.zero_grad()\n",
    "epochs=30\n",
    "batch_size=32\n",
    "for epoch in range(epochs):\n",
    "    total_loss=0\n",
    "    for idx in range(0, len(X_train), batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        hat_y = model(X_train[idx : idx+batch_size if idx+batch_size<=len(X_train) else len(X_train)])\n",
    "        loss=criterion(hat_y, y_train[idx : idx+batch_size if idx+batch_size<=len(X_train) else len(X_train)])\n",
    "        total_loss+=loss.item()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        hat_y_train = model(X_train)\n",
    "        hat_y_valid = model(X_valid)\n",
    "        train_acc=get_acc(hat_y_train, y_train)\n",
    "        valid_acc=get_acc(hat_y_valid, y_valid)\n",
    "        train_loss = criterion(hat_y_train, y_train)\n",
    "        valid_loss = criterion(hat_y_valid, y_valid)\n",
    "        print(\"{}/{} epoch train_loss:{:.4f} | train_acc:{:.4f} | valid_loss:{:.4f} | valid_acc:{:.4f}\".format(\n",
    "            epoch+1, epochs, train_loss, train_acc, valid_loss, valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#89\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertForPreTraining,BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0504 10:45:12.611952 140735748305792 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/shan/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0504 10:45:13.631198 140735748305792 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /Users/shan/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "I0504 10:45:13.635942 140735748305792 configuration_utils.py:168] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0504 10:45:14.703455 140735748305792 modeling_utils.py:337] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /Users/shan/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I0504 10:45:16.969186 140735748305792 modeling_utils.py:405] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0504 10:45:16.969978 140735748305792 modeling_utils.py:408] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# Encode text\n",
    "\n",
    "train_input_ids =pad_sequence([torch.Tensor(tokenizer.encode(i)) for i in train.TITLE.values], batch_first=True).long()  # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.\n",
    "valid_input_ids =pad_sequence([torch.Tensor(tokenizer.encode(i)) for i in valid.TITLE.values], batch_first=True).long() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 20%|██        | 1/5 [10:04<40:17, 604.38s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.196483850479126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|████      | 2/5 [21:14<31:12, 624.06s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.97632694244385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████    | 3/5 [32:35<21:22, 641.32s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.6916778087616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|████████  | 4/5 [43:56<10:53, 653.14s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.791101932525635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 5/5 [55:15<00:00, 663.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.43970739841461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.SGD(params=model.parameters() , lr=0.001)\n",
    "optimizer.zero_grad()\n",
    "epochs=5\n",
    "batch_size=64\n",
    "train_size = len(X_bert_train)\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    total_loss=0\n",
    "    for idx in range(0, train_size, batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        batch_x=train_input_ids[idx : idx+batch_size if idx+batch_size<=train_size else train_size]\n",
    "        batch_y=y_train[idx : idx+batch_size if idx+batch_size<=train_size else train_size]\n",
    "        loss, logit = model(input_ids=batch_x, labels=batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss.item()\n",
    "    print(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    loss, train_logit = model(input_ids=train_input_ids, labels=y_train)\n",
    "    loss, valid_logit = model(input_ids=valid_input_ids, labels=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc: 0.7572916666666667\n",
      "valid_acc: 0.7323232323232324\n"
     ]
    }
   ],
   "source": [
    "print(\"train_acc:\",get_acc(train_logit, y_train))\n",
    "print(\"valid_acc:\", get_acc(valid_logit, y_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
